{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multi-Nomial Naive-Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive-Bayes algorithm is used to classify strings such as emails and movie reviews into different classes.\\\n",
    "The datasets that we use are movie reviews made by a user and posted on IMDB.\\\n",
    "We then classify the movie reviews as either negative or positive.\\\n",
    "On github, I won't post the data used for this algorithm as there are in total 50 thousand movie reviews and It would be difficult to post all of them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive-Bayes algorithm works by considering, for eg an email, as a bag of words. We first remove redundant words such as \"the\", \"of\", etc and only keep usefull words which are called stopwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take an example of an email that just says \"Hello Friend\"\\\n",
    "We first calculate the probability for an email to be Spam ($P_{spam}$) and not spam ($P_{normal}$)\\\n",
    "we then calculate the probability of eachword occuring in a spam email ($P(Hello|Spam)$, $P(Friend|Spam)$) and of them being not spam ($P(Hello|Normal)$, $P(Friend|Normal)$) we then multiply the probabilities of the string of words being spam and the probability of a message being spam\n",
    "$$P(Hello,Friend|spam) = P(Hello|Spam)*P(Friend|Spam)*P_{spam}$$\n",
    "and the same for noraml\n",
    "$$P(Hello,Friend|normal) = P(Hello|normal)*P(Friend|Spam)*P_{normal}$$\n",
    "We then compare the two probabilties and assign our prediction to be the one with maximum probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem that can occur is, if a word comes up that is in the Normal Email bag of words but not in the Spam email bag of words. then probability of that word occuring in spam is 0 making our final answer for spam to be 0. To prevent this we add $\\alpha$ to the count of all words, $\\alpha$ is generally taken to be 1 but can be taken as any number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As probabilities can get very small when we have a huge bag of words. we use log(p) instead and add the log of probablitis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection\n",
    "from sklearn import datasets,preprocessing\n",
    "import sklearn\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is to get the required number of positive and negative reviews from the training and testing datasets. It returns the dataset with mentioned length and also a \"vocab\" list which has all the stop words that come up in out training data (both positive and negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(r\"[._;:!`Â¦\\'?,\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(r\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = REPLACE_NO_SPACE.sub(\"\", text)\n",
    "    text = REPLACE_WITH_SPACE.sub(\" \", text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    return [w for w in words if w not in stop_words]\n",
    "\n",
    "def load_training_set(percentage_positives, percentage_negatives):\n",
    "    vocab = set()\n",
    "    positive_instances = []\n",
    "    negative_instances = []\n",
    "    for filename in glob.glob(r'train/pos/*.txt'):  # Adjust path as needed\n",
    "        if random.random() > percentage_positives:\n",
    "            continue\n",
    "        with open(os.path.join(os.getcwd(), filename), 'r',encoding='utf-8') as f:\n",
    "            contents = f.read()\n",
    "            contents = preprocess_text(contents)\n",
    "            positive_instances.append(contents)\n",
    "            vocab = vocab.union(set(contents))\n",
    "    for filename in glob.glob(r'train/neg/*.txt'):  # Adjust path as needed\n",
    "        if random.random() > percentage_negatives:\n",
    "            continue\n",
    "        with open(os.path.join(os.getcwd(), filename), 'r',encoding='utf-8') as f:\n",
    "            contents = f.read()\n",
    "            contents = preprocess_text(contents)\n",
    "            negative_instances.append(contents)\n",
    "            vocab = vocab.union(set(contents))\n",
    "    return positive_instances, negative_instances, vocab\n",
    "\n",
    "def load_test_set(percentage_positives, percentage_negatives):\n",
    "    positive_instances = []\n",
    "    negative_instances = []\n",
    "    for filename in glob.glob(r'test/pos/*.txt'):  # Adjust path as needed\n",
    "        if random.random() > percentage_positives:\n",
    "            continue\n",
    "        with open(os.path.join(os.getcwd(), filename), 'r', encoding= 'utf-8') as f:\n",
    "            contents = f.read()\n",
    "            contents = preprocess_text(contents)\n",
    "            positive_instances.append(contents)\n",
    "    for filename in glob.glob(r'test/neg/*.txt'):  # Adjust path as needed\n",
    "        if random.random() > percentage_negatives:\n",
    "            continue\n",
    "        with open(os.path.join(os.getcwd(), filename), 'r', encoding= 'utf-8') as f:\n",
    "            contents = f.read()\n",
    "            contents = preprocess_text(contents)\n",
    "            negative_instances.append(contents)\n",
    "    return positive_instances, negative_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get half of the training data\n",
    "pos_training,neg_training,vocab = load_training_set(0.5,0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get half of the testing data\n",
    "pos_test,neg_test = load_test_set(0.5,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the vocab variable a list\n",
    "vocab = list(vocab)\n",
    "\n",
    "# Initiallize to dictionaries which would be used to keep count of all the words\n",
    "pos_vocab_count = {word: 0 for word in vocab}\n",
    "neg_vocab_count = {word: 0 for word in vocab}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the general probabilities of a movie review being negative or positive\n",
    "prob_of_pos = len(pos_training)/(len(pos_training)+len(neg_training))\n",
    "prob_of_neg = len(neg_training)/(len(pos_training)+len(neg_training))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initiallize a counter \n",
    "word_counts1 = Counter()\n",
    "\n",
    "# go through all positive training data and count the words\n",
    "for instance in pos_training:\n",
    "    word_counts1.update(instance)\n",
    "    \n",
    "\n",
    "\n",
    "for keys in word_counts1.keys():\n",
    "    pos_vocab_count[keys] = word_counts1[keys]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for negative training data\n",
    "word_counts2 = Counter()\n",
    "\n",
    "for instance in neg_training:\n",
    "    word_counts2.update(instance)\n",
    "\n",
    "\n",
    "for keys in word_counts2.keys():\n",
    "    neg_vocab_count[keys] = word_counts2[keys]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiallize lists for efficiency of predicting positive and negative reviews\n",
    "pos_efficiencies = []\n",
    "neg_efficiencies = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all vocab counts by 1, i.e. alpha is 1\n",
    "for words in pos_vocab_count:\n",
    "    pos_vocab_count[words] += 1\n",
    "for words in neg_vocab_count:\n",
    "    neg_vocab_count[words] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total mumber of word count in positive and negative reviews\n",
    "sum_of_pos = sum(pos_vocab_count.values())\n",
    "sum_of_neg = sum(neg_vocab_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiallize to dictionaries to store the probabilities of words in positve and negative reviews\n",
    "prob_of_word_pos = {word:0 for word in vocab}\n",
    "prob_of_word_neg = {word:0 for word in vocab}\n",
    "\n",
    "\n",
    "# Calculate the probabilities of each word in positive and negative reviews\n",
    "for keys in pos_vocab_count.keys():\n",
    "    \n",
    "    prob = (pos_vocab_count[keys])/(sum_of_pos)\n",
    "    prob_of_word_pos[keys] = prob\n",
    "\n",
    "for keys in neg_vocab_count.keys():\n",
    "    \n",
    "    prob = (neg_vocab_count[keys])/(sum_of_neg)\n",
    "    prob_of_word_neg[keys] = prob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficiency of the model in predicting negative reviews:  0.8763052208835341\n"
     ]
    }
   ],
   "source": [
    "# Initialize and answer list\n",
    "answer = []\n",
    "\n",
    "# Test neagtive reviews testing data\n",
    "for i in range(len(neg_test)):\n",
    "        # Initialize log probabilities as 0 for positive and negative reviews\n",
    "        logprobpos = 0\n",
    "        logprobneg = 0\n",
    "\n",
    "        # Iterate over each word of negative reviews and calculate the probablities\n",
    "        for word in neg_test[i]:\n",
    "            if  word in pos_vocab_count.keys():\n",
    "                logprobpos = np.log(prob_of_word_pos[word]) + logprobpos\n",
    "                logprobneg = np.log(prob_of_word_neg[word]) + logprobneg \n",
    "            \n",
    "            \n",
    "        # add the log of general probablities\n",
    "        actual_prob_pos = logprobpos + np.log(prob_of_pos)\n",
    "        actual_prob_neg = logprobneg + np.log(prob_of_neg)\n",
    "\n",
    "        # Check which probablity is larger and predict the answer accordingly, 1 for pos and 0 for neg\n",
    "        if actual_prob_pos > actual_prob_neg:\n",
    "            answer.append(1)\n",
    "        elif actual_prob_pos < actual_prob_neg:\n",
    "            answer.append(0)\n",
    "\n",
    "        # If both probablities are equal then assign randomly\n",
    "        elif actual_prob_neg == actual_prob_pos:\n",
    "            answer.append(random.randint(0,1))\n",
    "            \n",
    "\n",
    "# Calculate efficiency \n",
    "efficiency = answer.count(0)/len(answer)\n",
    "\n",
    "\n",
    "print(\"Efficiency of the model in predicting negative reviews: \",efficiency)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficiency of the model in predicting negative reviews:  0.7559916358372205\n"
     ]
    }
   ],
   "source": [
    "answerpos = []\n",
    "for i in range(len(pos_test)):\n",
    "        logprobpos = 0\n",
    "        logprobneg = 0\n",
    "        for word in pos_test[i]:\n",
    "            if  word in pos_vocab_count.keys():\n",
    "                logprobpos = np.log(prob_of_word_pos[word]) + logprobpos\n",
    "                logprobneg = np.log(prob_of_word_neg[word]) + logprobneg \n",
    "            \n",
    "            \n",
    "\n",
    "        actual_prob_pos = logprobpos + np.log(prob_of_pos)\n",
    "        actual_prob_neg = logprobneg + np.log(prob_of_neg)\n",
    "\n",
    "        if actual_prob_pos > actual_prob_neg:\n",
    "            answerpos.append(1)\n",
    "        elif actual_prob_pos < actual_prob_neg:\n",
    "            answerpos.append(0)\n",
    "        elif actual_prob_neg == actual_prob_pos:\n",
    "            answerpos.append(random.randint(0,1))\n",
    "            \n",
    "\n",
    "\n",
    "efficiencypos = answerpos.count(1)/len(answerpos)\n",
    "\n",
    "\n",
    "print(\"Efficiency of the model in predicting negative reviews: \",efficiencypos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make the confusion matrix of our model.\\\n",
    "For our data the confusion matrix would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['True positives' 'False negatives']\n",
      " ['false positives' 'True negatives']]\n"
     ]
    }
   ],
   "source": [
    "example_matrix= np.array([[\"True positives\",\"False negatives\"],[\"false positives\",\"True negatives\"]])\n",
    "print(example_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusuion_matrix = [[efficiencypos*len(answerpos), (1-efficiencypos)*len(answerpos)],[(1-efficiency)*len(answer) ,efficiency*len(answer)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4700. 1517.]\n",
      " [ 770. 5455.]]\n"
     ]
    }
   ],
   "source": [
    "confusuion_matrix = np.array(confusuion_matrix)\n",
    "print(confusuion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8161871081819643\n",
      "Precision:  0.8592321755027422\n",
      "Recall:  0.7559916358372205\n"
     ]
    }
   ],
   "source": [
    "## precision is TP/(TP + FP) \n",
    "## recall is TP/(TP + FN)\n",
    "## accuracy = (TP + TN)/(TP + FP + TN + FN)\n",
    "\n",
    "accuracy = (confusuion_matrix[0][0] +confusuion_matrix[1][1])/(confusuion_matrix[0][0] + confusuion_matrix[0][1] + confusuion_matrix[1][0] + confusuion_matrix[1][1])\n",
    "\n",
    "precision = confusuion_matrix[0][0]/(confusuion_matrix[0][0] + confusuion_matrix[1][0])\n",
    "\n",
    "recall =  confusuion_matrix[0][0]/(confusuion_matrix[0][0] + confusuion_matrix[0][1])\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is important when False positives are more costly than false negatives.\\\n",
    "Recall is used when False negatives are more costly than False positives.\\\n",
    "Accuracy is used when both are equally costly.\\\n",
    "In our data, precision is more important as recommending a user a movie due to a false positive review is more costly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The naivety of Naive-Bayer algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negatives of the Naive-Bayes algorithm is that the model considers all strings as just a bag of words and nothing more.\\\n",
    "For eg, in our eg of emails, the model would consider \"Hello Friend\" and \"Friend Hello\" as the same thing. It cannot distinguish between sentence structure or grammer. Also, as we remove redundant words such as \"the\", an email which says \"the the the the\" would just be a blank email and hence the model would break down. We can solve this edge case by having a default classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully made a Naive-Bayes algorithm to classify a movie review as either positive or negative. And also understood the shortcomings of our model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
